{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Env Set Up #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import gc as G\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from numba import cuda\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K, mixed_precision\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras_tuner import BayesianOptimization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau as RLOP, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier as KC\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from utils import view_train_images, plot_training_results, fast_import2 as FI2, get_images\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Using device: /GPU:0\n",
      "Set TensorFlow to use max GPU memory.\n"
     ]
    }
   ],
   "source": [
    "# limit usage of GPU memory\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Prevents TensorFlow from allocating all memory at once\n",
    "            print(f\"Enabled memory growth for: {gpu}\")\n",
    "\n",
    "        # Set GPU device\n",
    "        device = \"/GPU:0\"\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    device = \"/CPU:0\"\n",
    "    print(\"No GPU detected, using CPU.\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# set gpu to use max memory\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=3072)]  # Set in MB (e.g., 10GB)\n",
    "            )\n",
    "        print(\"Set TensorFlow to use max GPU memory.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Import #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "train_dir = \"C:/Users/RoiMinuit/Desktop/data/ILSVRC/Data/CLS-LOC/train\"\n",
    "val_dir = \"C:/Users/RoiMinuit/Desktop/data/ILSVRC/Data/CLS-LOC/val\"\n",
    "test_dir = \"C:/Users/RoiMinuit/Desktop/data/ILSVRC/Data/CLS-LOC/test\"\n",
    "\n",
    "# path to save tuner results\n",
    "kt_path = \"C:/Users/RoiMinuit/Desktop/data/ILSVRC/Tuning_Res/KT\"\n",
    "\n",
    "# save best model\n",
    "model_path = \"C:/Users/RoiMinuit/Desktop/data/ILSVRC/Tuning_Res/KT/Model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function processed 25000 images in 25 seconds.\n",
      "\n",
      "Function processed 3750 images in 10 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data import\n",
    "train_images, train_labels, val_images, val_labels = get_images(train_dir, train_dir, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "X_train = np.array(train_images)\n",
    "X_train = np.repeat(X_train, 3, -1)\n",
    "\n",
    "Y_train = [str(s) for s in train_labels]\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_hot = to_categorical(Y_train, num_classes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare testing data\n",
    "X_val = np.array(val_images)\n",
    "X_val = np.repeat(X_val, 3, -1)\n",
    "\n",
    "Y_val = [str(s) for s in val_labels]\n",
    "label_encoder = LabelEncoder()\n",
    "Y_val = label_encoder.fit_transform(Y_val)\n",
    "Y_val_hot = to_categorical(Y_val, num_classes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(3750,)\n",
      "1000\n",
      "973\n"
     ]
    }
   ],
   "source": [
    "# check data shape\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "print(len(np.unique(Y_train)))\n",
    "print(len(np.unique(Y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Optimization #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Keras Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for tuning\n",
    "epx = 50\n",
    "batch_size = 8\n",
    "RSCV_opts = ['adam', 'rmsprop']\n",
    "krnl = (3, 3)\n",
    "in_shape = (224, 224, 3)\n",
    "RSCV_activation = ['relu', 'tanh', 'sigmoid']\n",
    "RSCV_node1 = [2, 4, 8, 16, 32]\n",
    "RSCV_node2 = [2, 4, 8, 16, 32]\n",
    "RSCV_drop = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "RSCV_bacth = [2, 4, 6, 8, 16, 32, 64, 128]\n",
    "\n",
    "params_grid = dict(node1 = RSCV_node1,\n",
    "               node2 = RSCV_node2,\n",
    "               activations = RSCV_activation,\n",
    "               optimizers = RSCV_opts,\n",
    "               batch_size = RSCV_bacth,\n",
    "               drops = RSCV_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Classifier needs a model builder function to instantiate a new model for each new grid point\n",
    "def build_CNN(optimizers, node1, activations, node2, drops):\n",
    "    CNN = Sequential([\n",
    "        # first layer\n",
    "        Conv2D(node1, kernel_size=krnl, activation=activations, input_shape=in_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        # 2nd layer\n",
    "        Conv2D(node2, kernel_size=krnl, activation=activations),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        # output layer\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(drops),\n",
    "        Dense(1000, 'softmax')\n",
    "    ])\n",
    "    CNN.compile(optimizers,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # return built model\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Keras Classifier\n",
    "RS_cnn = KC(build_fn=build_CNN,\n",
    "            verbose=1,\n",
    "            epochs=5,\n",
    "            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish search space and search\n",
    "grid_search = RSCV(RS_cnn,\n",
    "                   param_distributions=params_grid,\n",
    "                   cv=5,\n",
    "                   scoring='accuracy',\n",
    "                   verbose=3)\n",
    "\n",
    "# execute search with categorical Y, not one-hotted\n",
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Keras Tuner ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for tuning\n",
    "activations = ['relu', 'leaky_relu', 'swish', 'elu']\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_path + '/ImgNet_Model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model for kt\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "def build_KT_CNN(hp):\n",
    "    layers = [\n",
    "        # input layer\n",
    "        Conv2D(hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "            input_shape=(128, 128, 3),\n",
    "            kernel_size=(3, 3),\n",
    "            activation=hp.Choice('activation', activations)),\n",
    "            Dropout(hp.Float('drop_rate', 0.1, 0.6))\n",
    "    ]\n",
    "    \n",
    "    # hidden layer(s)\n",
    "    for l in range(hp.Int('num_layers', 1, 5)):\n",
    "        layers.append(Conv2D(hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "                            kernel_size=(3, 3),\n",
    "                            activation=hp.Choice('activation', activations)))\n",
    "    \n",
    "    # output layer\n",
    "    layers.append(GlobalAveragePooling2D())\n",
    "    layers.append(Dropout(hp.Float('drop_rate', 0.1, 0.6)))\n",
    "    layers.append(Dense(1000, activation=hp.Choice('activation', activations)))\n",
    "    \n",
    "    CNN = Sequential(layers)\n",
    "    lr = hp.Float('learning_rate', min_value=1e-3, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    CNN.compile(optimizer=Adam(learning_rate=lr),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    # return built model\n",
    "    return CNN\n",
    "\n",
    "build_KT_CNN(kt.HyperParameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eastablish search space and search\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_KT_CNN,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=kt_path,\n",
    "    project_name='ImgNetOptimization'\n",
    ")\n",
    "\n",
    "# search\n",
    "tuner.search(X_train, Y_train,\n",
    "             batch_size=8,\n",
    "             epochs=10,\n",
    "             validation_data=(X_test, Y_test),\n",
    "             callbacks=[early_stop, checkpoint])\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view optimized model\n",
    "model = tuner.get_best_models()[0]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "G.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Bayesian Optimizer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for tuning\n",
    "activations = ['relu', 'leaky_relu', 'swish', 'elu']\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_path + '/ImgNet_Model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model for Bayesian\n",
    "def best_modHP(hp):\n",
    "    layers = [\n",
    "        # input layer\n",
    "        Conv2D(hp.Int('filters', min_value=8, max_value=128, step=8),\n",
    "            input_shape=(64, 64, 3),\n",
    "            kernel_size=(3, 3),\n",
    "            activation=hp.Choice('activation', activations)),\n",
    "            Dropout(hp.Float('drop_rate', 0.1, 0.6))\n",
    "    ]\n",
    "    \n",
    "    # hidden layer(s)\n",
    "    for l in range(hp.Int('num_layers', 1, 5)):\n",
    "        layers.append(Conv2D(hp.Int('filters', min_value=8, max_value=128, step=8),\n",
    "                            kernel_size=(3, 3),\n",
    "                            activation=hp.Choice('activation', activations))\n",
    "                            )\n",
    "        layers.append(BatchNormalization())\n",
    "        layers.append(MaxPooling2D(2, 2))\n",
    "        \n",
    "    \n",
    "    # output layer\n",
    "    layers.append(GlobalAveragePooling2D())\n",
    "    layers.append(Dropout(hp.Float('drop_rate', 0.1, 0.6)))\n",
    "    layers.append(Dense(1000, activation=hp.Choice('activation', activations)))\n",
    "    \n",
    "    CNN = Sequential(layers)\n",
    "    lr = hp.Float('learning_rate', min_value=1e-3, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    CNN.compile(optimizer=Adam(learning_rate=lr),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    # return built model\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "96                |96                |filters\n",
      "leaky_relu        |leaky_relu        |activation\n",
      "0.36209           |0.36209           |drop_rate\n",
      "1                 |1                 |num_layers\n",
      "0.0017833         |0.0017833         |learning_rate\n",
      "\n",
      "Epoch 1/20\n",
      "  5/782 [..............................] - ETA: 29s - loss: 16.1068 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.0176s). Check your callbacks.\n",
      "782/782 [==============================] - 32s 39ms/step - loss: 7.2104 - accuracy: 0.0011 - val_loss: 11.2045 - val_accuracy: 5.3333e-04\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9110 - accuracy: 8.4000e-04 - val_loss: 9.3220 - val_accuracy: 5.3333e-04\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9153 - accuracy: 0.0014 - val_loss: 9.4378 - val_accuracy: 0.0019\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 7.0812 - accuracy: 9.6000e-04 - val_loss: 11.9844 - val_accuracy: 8.0000e-04\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9471 - accuracy: 0.0011 - val_loss: 8.1764 - val_accuracy: 5.3333e-04\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9620 - accuracy: 7.6000e-04 - val_loss: 8.7320 - val_accuracy: 0.0016\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9438 - accuracy: 0.0014 - val_loss: 8.0139 - val_accuracy: 0.0013\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9154 - accuracy: 0.0011 - val_loss: 8.6123 - val_accuracy: 0.0013\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9178 - accuracy: 9.6000e-04 - val_loss: 7.7599 - val_accuracy: 0.0016\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9104 - accuracy: 7.2000e-04 - val_loss: 7.6570 - val_accuracy: 0.0013\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 6.9099 - accuracy: 0.0010 - val_loss: 7.5507 - val_accuracy: 0.0013\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 6.9089 - accuracy: 0.0010 - val_loss: 7.7654 - val_accuracy: 0.0019\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 6.9089 - accuracy: 0.0011 - val_loss: 8.1881 - val_accuracy: 0.0011\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 6.9083 - accuracy: 8.8000e-04 - val_loss: 7.5132 - val_accuracy: 0.0032\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 6.9111 - accuracy: 8.0000e-04 - val_loss: 7.6743 - val_accuracy: 5.3333e-04\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 6.9077 - accuracy: 8.4000e-04 - val_loss: 7.6868 - val_accuracy: 5.3333e-04\n",
      "Epoch 17/20\n",
      "781/782 [============================>.] - ETA: 0s - loss: 6.9082 - accuracy: 9.2029e-04"
     ]
    }
   ],
   "source": [
    "# define search space and search\n",
    "tuner_bay = BayesianOptimization(best_modHP,\n",
    "                                 objective='val_acc',\n",
    "                                 max_trials=5,\n",
    "                                 executions_per_trial=1,\n",
    "                                 project_name='Bayesian',\n",
    "                                 overwrite=True)\n",
    "\n",
    "# search\n",
    "tuner_bay.search(X_train, Y_train,\n",
    "                 epochs=20,\n",
    "                 validation_data=(X_val, Y_val),\n",
    "                 callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear gpu\n",
    "K.clear_session()\n",
    "G.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
